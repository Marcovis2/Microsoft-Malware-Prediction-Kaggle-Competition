{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the datasets\n",
    "X_train = pd.read_csv(\"X_train.csv\")\n",
    "y_train = pd.read_csv(\"y_train.csv\")\n",
    "X_test = pd.read_csv(\"X_test.csv\")\n",
    "y_test = pd.read_csv(\"y_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=6, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "\n",
      "Optimized Model\n",
      "------\n",
      "Final accuracy score on the testing data: 0.6132\n",
      "Final F-score on the testing data: 0.6145\n"
     ]
    }
   ],
   "source": [
    "# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data:\n",
    "# sample_size = len(X_train)                      # 100% of the entire training set\n",
    "sample_size = int(float(len(X_train)) * 0.01)     # 1% of the entire training set\n",
    "\n",
    "# TODO: Initialize the classifier\n",
    "clf = xgb.XGBClassifier(random_state=0)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "parameters = {'max_depth': [2,6]}\n",
    "\n",
    "\n",
    "# A parameter grid for XGBoost\n",
    "# params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "# 'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\n",
    "\n",
    "\n",
    "# TODO: Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "cv = ShuffleSplit(n_splits = 10, test_size = 0.20, random_state = 0)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(clf, parameters, scoring=scorer, cv=cv)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "print(best_clf)\n",
    "\n",
    "# Make predictions\n",
    "best_predictions = best_clf.predict(X_test)\n",
    "\n",
    "# Report\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Training Time: 75.21643114089966 seconds\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# clf = xgb.XGBClassifier(max_depth=2,\n",
    "#                         n_estimators=1000,\n",
    "#                         colsample_bytree=0.2,\n",
    "#                         learning_rate=0.1,\n",
    "#                         objective='binary:logistic',\n",
    "#                         updater='grow_gpu',\n",
    "#                         n_jobs=-1)\n",
    "\n",
    "# Specify sufficient boosting iterations to reach a minimum\n",
    "num_round = 4\n",
    "\n",
    "# Leave most parameters as default\n",
    "param = {'base_score':0.5, \n",
    "         'booster':'gbtree', \n",
    "         'colsample_bylevel':1,\n",
    "         'colsample_bytree':1,\n",
    "         'gamma':0, \n",
    "         'learning_rate':0.1, \n",
    "         'max_delta_step':0,\n",
    "         'max_depth':6, \n",
    "         'min_child_weight':1,\n",
    "         'n_estimators':2000,\n",
    "         'n_jobs':1, \n",
    "         'objective':'binary:logistic', \n",
    "         'random_state':0,\n",
    "         'reg_alpha':0,\n",
    "         'reg_lambda':1,\n",
    "         'scale_pos_weight':1,\n",
    "         'subsample':1,\n",
    "         'tree_method':'gpu_hist' \n",
    "         }\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data:\n",
    "# sample_size = len(X_train)                        # 100% of the entire training set\n",
    "sample_size = int(float(len(X_train)) * 0.4)        # 40% of the entire training set\n",
    "\n",
    "# Fit the learner to the training data using slicing with 'sample_size':\n",
    "\n",
    "# Convert input data from numpy to XGBoost format\n",
    "dtrain = xgb.DMatrix(X_train[:sample_size], label=y_train[:sample_size])\n",
    "\n",
    "tmp = time()\n",
    "# Train model\n",
    "learner = xgb.train(param, dtrain, num_round)\n",
    "print(\"GPU Training Time: %s seconds\" % (str(time() - tmp)))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'finalized_model3.sav'\n",
    "pickle.dump(learner, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions on the test set(X_test), then get predictions on the first 300 training samples\n",
    "\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "start = time() # Get start time\n",
    "predictions_test = learner.predict(dtest)\n",
    "end = time() # Get end time\n",
    "    \n",
    "# Calculate the total prediction time\n",
    "pred_time = end - start\n",
    "print(pred_time)\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "        \n",
    "# Compute accuracy on test set using accuracy_score()\n",
    "acc_test = accuracy_score(y_test, predictions_test)\n",
    "print(acc_test)\n",
    "        \n",
    "# Compute F-score on the test set which is y_test\n",
    "f_test = fbeta_score(y_test, predictions_test, 0.5)\n",
    "print(f_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
